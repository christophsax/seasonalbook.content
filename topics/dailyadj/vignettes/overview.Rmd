---
title: "Seasonal Adjustment of Daily Time Series in R: An Overview"
author:
  # see ?rjournal_article for more information
  - name: Christoph Sax
    affiliation: University of Base
    address:
    - line 1
    - line 2
    url: https://journal.r-project.org
    orcid: 0000-0002-9079-593X
    email:  author1@work
  - name: Author Two
    url: https://journal.r-project.org
    email: author2@work
    orcid: 0000-0002-9079-593X
    affiliation: Affiliation 1
    address:
    - line 1 affiliation 1
    - line 2 affiliation 1
    affiliation2: Affiliation 2
    address2:
    - line 1 affiliation 2
    - line 2 affiliation 2
  - name: Author Three
    url: https://journal.r-project.org
    email: author3@work
    affiliation: Affiliation
    address:
    - line 1 affiliation
    - line 2 affiliation
abstract: >
  This [article/chapter/post] discusses available methods for high frequency seasonal adjustment in R. It focuses on the adjustment of daily data, but also includes examples of weekly data. We discuss the literature and provide an overview of the available methods, including user examples. The methods are evaluated in two different ways: First we perform an out-of-sample forecast evaluation. Second, we compare the adjusted series with the results of X13, an established method for seasonal adjustment of monthly data. Finally, we discuss some specific problems of seasonal adjustment.
preamble: |
  % Any extra LaTeX you need in the preamble

# per R journal requirement, the bib filename should be the same as the output
# tex file. Don't forget to rename the bib file and change this example value.
bibliography: overview.bib

output: rticles::rjournal_article
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  out.width = "100%"
)
library(dailyadj)
library(tsbox)
library(tidyverse)

```
## Introduction

Automated data processing and the Internet have brought an enormous increase in data that is processed on a high frequency, e.g., at a daily, hourly or even higher frequency.
While some higher frequency series have been used in the past (e.g., Fisher 1923, cited by Ladiray 2018) these series are much more abundant now.
X-13ARIMA-SEATS offers a well-tested and time proven way of adjusting monthly, quarterly (or bi-annual) series, but it cannot deal with data at a higher frequency.

This [article/chapter/post] discusses how to perform seasonal adjustment on a higher frequency. We focus on daily data, as this is the most common use case, but will briefly discuss some challenges involving weekly or intra-day adjustments.

Despite the large interest, there is not much consensus on the appropriate adjustment method for daily series. Adjusting daily series often involves a substantial amount of trial and error, subjective judgment and exploration. This [article/chapter/post] gives an overview of the tools that are currently (2021) available in R.

The focus is both on the usage and on the methodology. We try to give usage example for each method and try to cover them in a unified manner.

We first give a short methodologial overview ob daily seasonal adjustment and disuss some specifi problems that come with high frequency seasonal adjustment. We then discuss various R packages that can be used to perform high frequency seasonal adjustment. In the 'Evaluation' chapter, we compare the results of these packages, and give some advice on the selction of an optimal method.
Further topics of high frequency seasonal adjustment are discussed in the final chapter.

## Overview


## Problems of High Frequency Adjustments

Daily seasonal adjustment comes with a few challenges that are not present in lower frequency data.
Let's focus on daily traffic casualties.

First, daily data comes at multiple periodicities: There is an annual periodicity, such as the effect of weather conditions or holiday patterns.
Then there is a weekly periodicity. Casualties may be higher during the weekdays, due to increased work traffic. For some series, there may be also a monthly periodicity. If people get their salaries by the end of the month, they may be more likely to perform certain investments.

Second, many daily data series are available for a few years only. Whily, e.g., the SEATS adjustment requires a minimal series length of XX years, many daily series are shorter.

Third, higher frequency series are generally more volatile and prone to outliers.

Fourth, the effect of individual holiday is challenging to estimate. Often, economic effects of holidays may occur before or after a holiday, thus lagging or leading them is crucial.


## Parametric versus Non-parametric Models

Various attempts to seasonly adjust data can be broadly distinguished into parametric and non-parametric approaches.
Non-parametric approaches seem to be the more obvious candidates to use with the irregular structure of daily data. Parametric models require the time units to be regularly spaced.
Non-parametric estimation is also what is used in the X-11 method of X-13.

## R Packages

As mentioned before, there is no accepted consensus on how to perform daily seasonal adjustment.
In the following, we discuss various possibilities to adjust series in R.
We focus on a single time series, and describe the concrete steps that are required to perform an adjustment.

*dailyadj* is an R package to accompany this article. It can can be donloaded from GitHub [needs to be published.]. It contains various daily example series.
In the following, we focus on UK traffik casualities from 2005 to 2016.



```{r}
library(dailyadj)
library(tsbox)
library(tidyverse)

casualties
```

We store high frequency time series in an R data frame, specifically in a *tibble*. This data frame has a `time` and a `value` column.
This has two major advantages to other classes, such as `ts`. First, it does not force the data into any kind of regularity. February 29 data, if it orrucs, can be stored without problems. Second, R has offers a pleotora of tools to perform fantastic stuff with data frames, while we have less tools available for other classes. Here, and throughtout the article, we use *tsbox* and *tidyverse* to simplify the data manipulation. `ts_plot()` offers a convenient way to plot this tabular data (or any other time series data):

```{r}
ts_plot(casualties)
```


## ARIMA + Month, Weekday / Holiday Dummies

Perhaps the simplest way to seasonally adjust a high frequency time series is to use dummies to control for month or weekday effects. As we will see, these models have some considerable drawbacks, but their discussing may be helpful nevertheless.
Also, these kinds of adjustments are frequently found in the literature. E.g., timmermans 18, [lengwiler 20](https://sjes.springeropen.com/articles/10.1186/s41937-020-00052-y).

We start by constructing a dummy variable with weekday and monthly effects:

```{r, dummy-matrix, cache = TRUE}
dums <-
  casualties %>%
  mutate(wday = lubridate::wday(time, label = TRUE)) %>%
  mutate(month = lubridate::month(time, label = TRUE)) %>%
  select(time, wday, month) %>%
  fastDummies::dummy_cols("wday", remove_selected_columns = TRUE) %>%
  fastDummies::dummy_cols("month", remove_selected_columns = TRUE) %>%
  select(-wday_Mon, -month_Jan)

dums
```

These variables can be used as exogenous variables in a ARIMA model. We use `forecast::auto.arima()` to determine the ARMA order. Note that we do not want to use the seasonal part of the model, since we use dummies for this purpose.

```{r, arimax, cache = TRUE}

fit <- auto.arima(casualties$value, seasonal = FALSE, xreg = as.matrix(dums[, -1]))
adj <- casualties
adj$value <- as.numeric(fit$fitted)

ts_plot(casualties, adj)

```


The nice think about the dummy model is that its seasonal effects are very easy to interprete. By construction, they are constant over time, and can be visualized as follows:

```{r, coeff-plots}

enframe(coef(fit)) %>%
  filter(grepl("wday", name)) %>%
  mutate(name = gsub("wday_", "", name)) %>%
  mutate(name = factor(name, levels = unique(name))) %>%
  ggplot(aes(x = name, y = value)) +
    geom_col() +
    ggtitle("Weekday effects", subtitle = "Baseline: Monday")


enframe(coef(fit)) %>%
  filter(grepl("month", name)) %>%
  mutate(name = gsub("month_", "", name)) %>%
  mutate(name = factor(name, levels = unique(name))) %>%
  ggplot(aes(x = name, y = value)) +
    geom_col() +
    ggtitle("Month effects", subtitle = "Baseline: January")

```

We see that, on average, traffic casualties are lower on Sunday and peak on Friday. We also see that, on average, casualties are sligthly lower in early autumn.



## STL

While dummy models are easy to estimate and iterprete, they have a substantial drawback: Their effects are typically *time-invariant*, which is rarely a good assumption. Where we look at weekday or month effects, these effects are likely to change over time.

A straigthforward solution is to estimate these effects non-parametrically. Fundamentally, this uses the same idea as the ubiquitous X-11 method, wich is used by X13, the seasonal adjustment method by the US Census bureau.

STL (Cleveland et al, 1990, seasonal-trend decomposition procedure based on loess)
follows a similar idea but uses local regressions to estimate seasonal effects that may change over time. R base has a function `stl()` that performs this decompostion, but it requires the data to be equispaced. Models with an equispace requirement will be discusses later on.

However, STL can be applied to irregular data as well.
The basic idea is to align all weekdays and draw a smoothed line through these data points. This is you initial estimation of a *weekday* effect, which will be substracted from a detrended series. Secondly, the days of a month or a year may be aligned, leading an estimation of a monthly or yearly effect.
Once all effects are estimated, the procedure may be repeated, with an iteratively improved trend estimation.

*dailyseas* contains a simple implementation of STL that works in many circumstances. See the source code of `seas_daily()`, which uses relatively simple dplyr manipulations.


```{r, stl, cache = TRUE}

casualties %>%
  seas_daily() %>%
  ts_pick("orig", "adj") %>%
  ts_plot()

```

## dsa

In a very similar spirit, the dsa packages implements a version of STL.
It is computationally much more intensive, as it performs extensive outlier adjustments.

The following code autmatically decomposes `casulties`, using the *dsa* package:


```{r, dsa, cache = TRUE}

library(dsa)
z <- dsa::dsa(ts_xts(casualties))
plot(z, dy = FALSE)

```

## prophet

The *prophet* packages promises an automated approach to forecasting and decomposing high frequency time series. Like the procedures above, it does not require the data to be equispaced.
(Taylor SJ, Letham B. 2017)

At its core, the Prophet procedure is an additive regression model with four main components:

A trend, a yearly seasonal component, a weekly weekly seasonal component and a user-provided list of important holidays. This is fundamentally no different to the STL methods above. However, for trend estimation, prophet uses a piecewise linear or logistic growth curve trend, for yearl effect, is ueses Fourier series. As the dummy model in the beginning, it uses dummy variables to estimate the trend.

In order to prepare the data, column names need be changed to *prophet*'s defaults.
Estimation is specified using `prophet()`.
Country specific holiday effects can be added.
Acutal fitting is done by `fit.prophet()`:

```{r, prophet, cache = TRUE}
library(prophet)

df <- rename(casualties, ds = time, y = value)
m <-
  prophet(daily.seasonality = FALSE) %>%
  add_country_holidays(country_name = 'UK') %>%
  fit.prophet(df)

```

The seasonally decomposed series (and a forecast) can be extracted as follows:

```{r, prophet-components, cache = TRUE}
# not strictly needed, but will include forecast too
future <- make_future_dataframe(m, periods = 31)
forecast <- as_tibble(predict(m, future))

forecast %>%
  transmute(
    time = as.Date(ds),
    additive_terms,
    yhat
  ) %>%
  left_join(casualties, by = "time") %>%
  mutate(adj = value - additive_terms) %>%
  select(time, value, adj) %>%
  ts_long() %>%
  ts_plot()
```

## TBATS

The previous models did not require the data to be equispaced.
However, modelling techniques have been developed in the context of monthly or quarterly data. They often assume that each low frequency period must include the exact same number of high frequency periods.

With daily data (or much worse, weekly data, which will be discussed below), this is not the case. A year may have 365 or 366 days, and it is not always clear how the February 29 problem should be handled.

`ts_ts` from the tsbox package offers an easy way to convert daily data into regular `"ts"` objects with a frequency of 365.2425.
Thus, days are slightly offset in each year.

```{r, ts_ts}
x_ts <- ts_ts(casualties)
head(x_ts)
```

TBATS Models (De Livera, Hyndman, & Snyder, 2011) require the data to be equispaced.


```{r, tbats, cache = TRUE}

fit <- tbats(x_ts)
adj <- fit$fitted
ts_plot(casualties, adj)
```

## Evaluation


Which method should you choose? Various criterions could be applied. A seasonal decomposition model can be used to perform forecasts as well. The forecasting capability can be used for a pseudo out-of-sampe forcasting exercise. For all the methods discussed and for a number of time series, we perform such an exercise.

Also, a seasonally adjusted series can be compared ot established methods of seasonal adjustment for monthly and quarterly data. In a second excercise, we compare monthly values of adjusted series to adjustments of X-13, using the X-11 method.


## Out-of-sample forecast

As in (Timmermans 18), the following produces OOS forecasts for the twelve last months for UK traffic deaths. We apply `dailyseas::eval_oos()` to perform an OOS forecast evaluation for all models.

The full OOS results can be found [here](https://github.com/christophsax/x13book/blob/master/topics/dailyadj/vignettes/overview.md)

[I think the detailed OOS results are quite interesting, as they show you what a method 'gets' and what it does not.]

Here is the table for two series, showing the mean percentage deviation:


```{r, echo = FALSE, messages = FALSE}


env <- new.env()
load(file = here::here("script/oos_eval_summary_ans.RData"),  envir = env)

library(tidyverse)


summary_per_series <- function(x) {
  lapply(x, summary_oos_evals) %>%
  bind_rows(.id = "model") %>%
  filter(period == "Mean") %>%
  select(model, mpce)
}

bind_rows(lapply(as.list(env), summary_per_series), .id = "series") %>%
  mutate(series = gsub("z_", "", series)) %>%
  pivot_wider(names_from = "series", values_from = "mpce") %>%
  kable(
    format = "latex",
    digits = 2,
    booktabs = TRUE,
    linesep = ""
  )

```

## Comparison to X-13

For the methods and series above, we:

- compute adjusted series
- aggregate them to monthly
- compare them X13 Series
- compute mpce between the two and tabluate

[perhaps also look at `qs()`?]

there is a literature on this (montly GDP vs quarterly). If a monthly adjustment seems good but produces a bad quarterly adjustemnt, it is probably not a good adjustment. [James, perhaps elaborate?]


## Computation time

Finally, for practial purposes, speed may be an important consideration, too.
Differences in computational speed between the methods are large, ranging from less than a second to serveral minutes.

```
stl (my one): 0.639
seas_dummy:   2.319
prophet:     10.181
dsa:        145.553
```


## Conclusions

*prophet*, *dsa*, and the simple stl procedure all work relatively well. Working with equispaced data is much harder.

[Is this because I don't use the correctly? Should I just remove Feb. 29?
Grateful for a TBATS example that produces something meaningful]




# Other challenges


[Discuss some other challenges of daily seas adj]


## Short series

Many daily time series are short. What does it mean with respect to the discussion above. If I have only 3 years of data, which methods still work?


## Calendar effects

[Will try to improve my method on that, prophet and dsa do something about it.
Could provide an example, OOS comparison]


## Cross Seasonal Effects

If month effects (e.g., salary payment at the end of a month) collide with week effects (e.g., weekend), we can get some patterns that are very hard to model. How relevant is the problem? What should you be done about it?


## Series specific effects

Transaction series have an end of months effect (Timmermans 18), but we don't find it in other data. How to deal with such things?


## Weekly seasonality

One solution would be to disaggregate, perform daily seasonal adjustment and aggregate again. Could provide an example.

Compare to weekly methods?


# Conclusions




# Literature

Cleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. J. (1990). STL: A seasonal-trend decomposition procedure based on loess. Journal of Official Statistics, 6(1), 3–33.

Lengwiler, Y. Blacking out. Swiss J Economics Statistics 156, 7 (2020). https://doi.org/10.1186/s41937-020-00052-y

Taylor SJ, Letham B. 2017. Forecasting at scale. PeerJ Preprints 5:e3190v2 https://doi.org/10.7287/peerj.preprints.3190v2

De Livera, A. M., Hyndman, R. J., & Snyder, R. D. (2011). Forecasting time series with complex seasonal patterns using exponential smoothing. J American Statistical Association, 106(496), 1513–1527. https://robjhyndman.com/publications/complex-seasonality/

Timmermans, Monique, Ronald Heijmans, and Hennie Daniels. "Cyclical patterns in risk indicators based on financial market infrastructure transaction data." (2017).



# Appendix


### Components Plots

```{r, include = FALSE}
oos_prophet <- read_csv(here::here("script/data/oos_prophet.csv"), col_types = cols())
ans_prophet <- read_csv(here::here("script/data/ans_prophet.csv"), col_types = cols())

oos_daily <- read_csv(here::here("script/data/oos_daily.csv"), col_types = cols())
ans_daily <- read_csv(here::here("script/data/ans_daily.csv"), col_types = cols())

oos_dsa <- read_csv(here::here("script/data/oos_dsa.csv"), col_types = cols())
ans_dsa <- read_csv(here::here("script/data/ans_dsa.csv"), col_types = cols())

```


#### Casualties, Prophet

```{r, echo = FALSE, messages = FALSE}
plot_components(filter(ans_prophet, series == "casualties"))
```

#### Casualties, DSA

```{r, echo = FALSE, messages = FALSE}
plot_components(filter(ans_dsa, series == "casualties"))
```
#### Casualties, STL

```{r, echo = FALSE, messages = FALSE}
plot_components(filter(ans_daily, series == "casualties"))
```



### OOS Forecast plots


#### Casualties, Prophet


```{r, echo = FALSE, messages = FALSE}
plot_oos_evals(filter(oos_prophet, series == "casualties"))
```


#### Casualties, DSA


```{r, echo = FALSE, messages = FALSE}
plot_oos_evals(filter(oos_dsa, series == "casualties"))
```

#### Casualties, STL


```{r, echo = FALSE, messages = FALSE}
plot_oos_evals(filter(oos_daily, series == "casualties"))
```

