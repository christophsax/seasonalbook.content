# regARIMA Model {#sec-regarima}

```{r}
#| results: "asis"
#| echo: false
source("_common.R")
status("polishing", "2023-11-20", section = "regARIMA Model")  # drafting, polishing, complete
```


<!--

Style Guide ?

X-13
X-13ARIMA-SEATS
Titles in sentence style





 -->

An ARIMA model with external regressors (regARIMA) lies at the heart of many of the procedures in X-13. As we have seen in chapter FIXME, regARIMA consists of several interacting specs. Some of the these specs will be further detailed in later chapters (Outlier, Trading Days).

Like most statistical modeling in X-13, the process to find the best model is iterative. Analysts use statistical tools and their expertise to fine-tune the regARIMA model. This iterative process is visualized in the flow chart below.

```{mermaid}
%%| label: fig-arima-interact
%%| fig-cap: "Interaction between regARIMA, diagnostics and seasonal adjustment"

flowchart TD
  A[RegARIMA Modeling] --> B(Model Comparison / Diagnostics)
  B --> A
  A --> C[Seasonal Adjustment]
  C --> D[Seasonal Adjustment Diagnostics]
  D --> C
  D --> A
```

Once a satisfiying regARIMA has been found, one proceeds to the seasonal adjustment step. The regARIMA model is crucial in forecasting the time series for applying symmetric filters and extracting components, whether using SEATS or X-11 for seasonal adjustment. In addition, in SEATS, the ARIMA model is also used to derive trend, seasonal, and irregular components.

The regARIMA model also includes exogenous information or regressors, like holiday effects, additive outliers, and level shifts. These elements help in forecasting and analyzing the series. For example, regARIMA modeling can answer if a series is affected by trading day effects or how outliers impact results. It's also used to integrate external regression variables such as moving holidays and trading day effects into the analysis.


## ARIMA model overview

The regARIMA model combines two elements: **reg**ression and **ARIMA**. The ARIMA segment itself is composed of a differencing order (the "I", for "integrated") and the ARMA part. This chapter aims to simplify these concepts without getting too technical, providing enough understanding for effective seasonal adjustment.

ARIMA stands for Autoregressive Integrated Moving Average. "Autoregressive" (AR) means the model uses past values of the series to predict the current value. For instance, an AR(1) model uses one lagged value:

$$ Y_t = \phi Y_{t-1} + a_t $$

Here, $Y_t$ is the time series, $\phi$ a coefficient, and $\{a_t\}$ an uncorrelated sequence of errors.[^ar_p]

[^ar_p]: An AR(p) model extends this to $p$ lagged values:

    $$ Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \cdots + \phi_p Y_{t-p} + a_t $$

    where now we have $p$ coefficients $\phi_1, \phi_2, \ldots, \phi_p$ to be estimated.

The "Moving Average" (MA) part uses past errors of the series for prediction. An MA(1) model looks like:

$$ Y_t = a_t + \theta a_{t-1} $$

and uses the current and one lagged error term.[^ma_q]

[^ma_q]: An MA(q) model includes $q$ lagged error terms:

    $$ Y_t = a_t + \theta_1 a_{t-1} + \theta_2 a_{t-2} + \cdots + \theta_q a_{t-q} $$.

An ARMA(p, q) model combines these AR and MA components, using $p$ lagged values of the series and $q$ lagged error terms. In practice, X13's automatic modeling usually finds appropriate $p$ and $q$ values. However, it worth paying attention to correctly set the differencing and regression variables for seasonal adjustment.



## Differencing

ARMA models work best for stationary time series.
This means the mean does not depend on time (such as increasing trend) or have a correlation structure that changes.
Many techniques could be used to take a non-stationary time series and transform it to stationarity, one ubiquitous method is differencing.
There is a famous results that states if you difference your series $k$ times it will remove a polynomial trend of degree $k$.
Essentially, if you observe a time series with a linear trend then first differencing will remove the trend.
If a time series has quadratic trend (polynomial of order 2) then differencing twice will remove that trend.
A similar phenomenon can happen at seasonal lags and often a time series will also require seasonal differencing to reduce it to stationary.
The order of differencing, also called the intgreation order, for the non-seaosnal and seasonal parts of our model are usually notated as $d$ and $D$ respectively.
When we bring the integration order together with the stocastic model specification we have the notation $$\text{SARIMA}\underbrace{(p, d, q)}_{\text{non-seasonal }}\underbrace{(P, D, Q)}_{\text{seasonal}}.$$ This can be seen easily with an example.
Consider the log transformed `AirPassengers` series.

```{r}
plot(log(AirPassengers))
```

We see a clear increasing trend and seasonal pattern.
Let's call the observed series $X_t$.
We can difference the series to make $Y_t = \Delta X_t = X_t - X_{t-1}$.
A plot of $Y_t$ looks like

```{r}
plot(diff(log(AirPassengers)))
```

The trend has been removed however some seasonal trend (strong seasonal patterns) still exist.
Apply seasonal differencing to the already first differenced series $Y_t$:

$$ Z_t = Y_t - Y_{t-12} $$

A plot of $Z_t$:

```{r}
plot(diff(diff(log(AirPassengers)), 12))
```

Here we can see that both the original linear trend and seasonal pattern are removed and what is left is a stationary process that can adequately be modeled with an SARMA(0, 1)(0, 1) model.
When you bring in the integration (differencing) order of one for the seasonal and non-seasonal components, we are left with the model named after this exact time series!
The so called \`\`airline model'' is the SARIMA(0, 1, 1)(0, 1, 1) and the terminology came to popularity via Box and Jenkins, "Time Series Analysis, Forecasting and Control" textbook.

## Fitting SARIMA (optional)

Here we present a very oversimplied way to start to understand what values of $p, P, q$ and $Q$ you can investigate for your time series of interest.
Recall that earlier it was mentioned that using automatic model identification is sufficient for most to get an adequate seasonal adjustment.
Hence, this is simply for the interested reader to begin to gain additional intuition into the stochastic structures involved in their series and the types of structures the automatic modeling procedures look at.
One of the main tools in a time series analyist tool box is the autocorrelation function (ACF).
This is a function that returns the correlation between observations $h$ time units apart throughout the entire sample.
So for $h=2$ this means looking at the correlation between the pairs $(X_1, X_3), (X_2, X_4), (X_3, X_5), \ldots$.
Then a way to build a SARIMA model is to match the sample ACF and the theoretical ACF of a given model.
The main point distinguishing an AR($p$) and MA($q$) is how their theoretical ACF behaves.
An AR($p$) will have ACF the has exponential decay as $h$ increases.
For example, an AR(1) ACF is $$\rho(h) = \phi^h$$ An MA($q$) models ACF will be non-zero for the first $q$ lags and then cutoff to zero thereafter.
The ACF of an MA(1) is $$\rho(h) = \begin{cases} 
~~1 & h = 0 \\
\frac{\theta}{1 + \theta^2} & h = 1 \\
~~0 & \text{otherwise}
\end{cases}
$$ In practice of course the difference between decay and cut-off can be nebulous to detect but the interested reader is encouraged to explore the `arima.sim()` function the look at the sample ACF with the `acf()` function.
As you increase the sample size it will converge to the theoretical ACF value and you can start to see the structures just discussed.

```{r}
x_AR <- arima.sim(model = list(ar = .75), n = 300)
x_MA <- arima.sim(model = list(ma = .75), n = 300)
tsbox::ts_plot(cbind(x_AR, x_MA))
op <- par(mfrow = c(1, 2), mar = c(5, 2, 4, 2))
acf(x_AR, xlab = "h", main = ""); title("ACF of AR(1) model")
acf(x_MA, xlab = "h", main = ""); title("ACF of MA(1) model")
par <- op
```

## Regression

We have discussed SARIMA modeling (both the SARMA and differencing), now we see how exogenous regression variables come into play.

The regARIMA model takes the form $$ f\left(\frac{Y_t}{D_t} \right) = \boldsymbol{\beta}^\prime {\mathbf X}_t + Z_t .$$ Here $Y_t$ is the observed time series.
The function $f$ represents a transformation, most commonly used is the log transform ie $f(x) = \log(x)$.
$D_t$ is any intervention that has taken place prior to any transformation or modeling.
This intervention is usually subjective and customized for individual series on an as-needed basis.
For example, if a soybean farmer strike occurred and the soybean export series suffered for its duration.
This type of event might adversely affect the seasonal adjustment filters and automatic model identification routines and can be mediated as an initial step.
If no transformation or intervention is needed the model form is: $$ Y_t = \underbrace{\boldsymbol{\beta}^\prime {\mathbf X}_t}_{\text{Regression}} + \underbrace{Z_t}_{\text{ARIMA}} .$$

The regression variables appear in the columns of the design matrix ${\mathbf X}_t$ and $Z_t$ is an ARIMA process.
This last assumption on $Z_t$ is what distinguished a regARIMA model from more classic linear models and multiple linear regression where error terms are assumed uncorrelated.

In order to achieve a suitable seasonal adjustment it is important to get the regARIMA model correct.
For most dataset the built in automatic modeling features of the X13 program will be suitable to detect a reasonable model.
This can be used as a starting point for more rigorous regARIMA model development or used as the final regARIMA modeling choice for your seasonal adjustment needs.
We evoke automatic model identification through the XXX spec.
The default behavior of the R seasonal package is XXX which includes automatic model identification.

::: callout-tip
## Automatic and manual model choice

As an aside, the general rule is to not use automatic modeling in production.
This mean, if you are going to include seasonal adjustment as part of a large scale data processing that occurs regularly (say monthly), then it is not advisable to have automatic model identification run every month.
Instead, an alternative process, is to run automodel once and then fix the model choice in the XXX spec file.
This does not need to be done manually since the `static()` function from the seasonal package can do this for you.
:::

| Outlier Type                | Automatic Detection Available? |
|-----------------------------|--------------------------------|
| Additive outliers (AO)      | Yes (default)                  |
| Level shifts (LS)           | Yes (default)                  |
| Temporary level shifts (TL) | Yes                            |
| Temporary changes (TC)      | No                             |
| Ramps (RP, QI, QD)          | No                             |
| Seasonal outliers (SO)      | No                             |

## Case Study: AirPassengers

Consider the default seasonal adjustment:

```{r}
library(seasonal)
m <- seasonal::seas(AirPassengers, x11 = "")
print(m$spc$automdl)
print(m$spc$arima)
```

Notice the value `NULL` indicates no ARIMA model is specified and the returned arguments for the automdl spec indicate it is active during the X13 run.

```{r}
seasonal::udg(m, "automdl")
```

Indicates that automatic modeling identified the (0 1 1)(0 1 1) model as the best choice.
If we want to hardcode this model for subsequent runs, and turn off automatic model identification, this can be done via

```{r}
m_call <- seasonal::static(m)
m2 <- eval(m_call)
```

There are many options you can modify when searching for outliers in your series.
Some of the most practical options to start your exploration are the *type*, *critical value* and *span* that you would like to search.

Here is an example of using span to limit the outlier search to the last few years of a series:

```{r}
m_span <- seas(AirPassengers,
  outlier.types = c("ao", "ls", "tc"),
  outlier.critical = 4.0,
  outlier.span = "1958.jan, ")
summary(m_span)
```

```{r}
m_nospan <- seas(AirPassengers,
  outlier.types = c("ao", "ls", "tc"),
  outlier.critical = 4.0)
summary(m_nospan)
```

The default critical value is set based on the length of the outlier span.
Notice the MA-Nonseasonal-01 value when comparing `m_span` with `m_nospan`.
We see the choice of span, and ultimately the choise to include an outlier in your model can have a dramatic effect on the estimated regARIMA parameters.

<!-- ## Case study 2 -->

<!-- Decide if you should include AO in May 2014. -->

<!-- Construct a simple user defined regressor to handle specific issue. -->
